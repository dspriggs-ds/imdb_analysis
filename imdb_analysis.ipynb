{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1868cbc-ac9d-4cea-b7c5-013b09400954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üé¨ IMDb Data Analysis ‚Äì Dataset Creation Overview\n",
    "\n",
    "This notebook outlines the process of creating a comprehensive dataset for analyzing movie and television data from IMDb.\n",
    "\n",
    "## üì¶ Data Sources\n",
    "\n",
    "1. **IMDb Datasets**  \n",
    "   Source: [https://datasets.imdbws.com/](https://datasets.imdbws.com/)  \n",
    "   These datasets are updated daily and include comprehensive information about:\n",
    "   - Titles (`title.basics.tsv.gz`)\n",
    "   - Ratings (`title.ratings.tsv.gz`)\n",
    "   - Crew and principal cast (`title.crew.tsv.gz`, `title.principals.tsv.gz`)\n",
    "   - Box office and production details\n",
    "\n",
    "2. **Box Office Mojo API (via `boxoffice_api`)**  \n",
    "   This Python package allows retrieval of box office performance data, including:\n",
    "   - Domestic and international grosses\n",
    "   - Opening weekend performance\n",
    "   - Distributor metadata\n",
    "\n",
    "## üõ†Ô∏è Dataset Creation Workflow\n",
    "\n",
    "- **Step 1:** Download and extract IMDb `.tsv.gz` files into structured DataFrames\n",
    "- **Step 2:** Clean and filter relevant records (e.g., movies only, specific years, non-null ratings)\n",
    "- **Step 3:** Enrich the dataset by calling the `boxoffice_api` to pull gross revenue and studio metadata\n",
    "- **Step 4:** Merge datasets to form a unified view of titles with both IMDb ratings and box office performance\n",
    "- **Step 5:** Store as a clean Delta or Parquet table for downstream analysis in Databricks or PySpark\n",
    "\n",
    "> ‚ö†Ô∏è Note: Ensure compliance with IMDb's [terms of use](https://www.imdb.com/conditions) when using and distributing the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "This dataset will serve as the foundation for analyzing trends in movie ratings, revenue, and genres with Microsoft Power BI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6473428c-7d72-47bd-963b-424787d219cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install BeautifulSoup Library"
    }
   },
   "outputs": [],
   "source": [
    "%pip install bs4  # Install BeautifulSoup library for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d444d798-3013-49dd-96dd-276049375bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install boxoffice-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c9fdc5-137a-40ec-9c9e-af34ca466230",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "# Restart the Python environment to apply changes made by library installations or updates\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e250ca26-99ae-42f7-91fb-72f06f153fb4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Required Libraries for Data Processing"
    }
   },
   "outputs": [],
   "source": [
    "import json  # For handling JSON data\n",
    "import os  # For interacting with the operating system\n",
    "import requests  # For making HTTP requests\n",
    "import urllib.request  # For opening and reading URLs\n",
    "from boxoffice_api import BoxOffice\n",
    "from bs4 import BeautifulSoup  # For parsing HTML and XML documents\n",
    "from delta.tables import DeltaTable  # For working with Delta Lake tables\n",
    "from pyspark.sql import Row  # For creating Spark DataFrame rows\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13a53b6-d403-4803-be22-d2548a1e28c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Retrieve OMDB API Key from Secrets Manager"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the OMDB API key from Databricks secrets\n",
    "omdbkey = dbutils.secrets.get(scope = \"djsdbsecrets\", key = \"omdbapikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a34129e-b08a-4551-a19a-09aef92e371d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Database Catalog and Schema"
    }
   },
   "outputs": [],
   "source": [
    "ext_df = spark.sql('DESCRIBE EXTERNAL LOCATION `externaloc`')  # Retrieve metadata for the external location\n",
    "ext_loc = ext_df.select('url').collect()[0][0]  # Extract the URL of the external location from the DataFrame\n",
    "catalog_name = \"data_analysis\"  # Catalog name for the database\n",
    "schema_name = \"imdb_data\"  # Schema name for the database\n",
    "# Create a catalog if it does not exist, specifying a managed location\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS {0} MANAGED LOCATION '{1}'\".format(catalog_name, ext_loc))\n",
    "\n",
    "# Create a schema within the specified catalog if it does not exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS {0}.{1}\".format(catalog_name, schema_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a3dd2d-afc4-4c27-bce1-27c0bda7635a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set IMDB Download Path and URL"
    }
   },
   "outputs": [],
   "source": [
    "download_path = '/Volumes/generaldata/dataanalysis/upload/imdb/'  # Path to download IMDB data\n",
    "url = 'https://datasets.imdbws.com/'  # URL for IMDB datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3310efb2-ad4e-4b99-9ebb-624712268ca8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download and Decompress Files from HTML List"
    }
   },
   "outputs": [],
   "source": [
    "html_content = requests.get(url).text  # Fetch HTML content from the specified URL\n",
    "soup = BeautifulSoup(html_content, 'html.parser')  # Parse the HTML content using BeautifulSoup\n",
    "items_list = soup.find('ul')  # Locate the unordered list in the parsed HTML\n",
    "for item in items_list.findAll('a'):  # Iterate through all anchor tags within the list\n",
    "   file_name = item.getText()  # Extract the text (file name) from the anchor tag\n",
    "   decompressed_file_name = file_name.replace('.gz', '')  # Remove the .gz extension for the decompressed file name\n",
    "   file_path = item.get('href')  # Get the href attribute (URL) of the anchor tag\n",
    "   dest_download_path = \"/tmp/{}\".format(file_name)  # Define the temporary download path for the file\n",
    "   urllib.request.urlretrieve(file_path, dest_download_path)  # Download the file to the temporary path\n",
    "   os.system('gzip -d {}'.format(dest_download_path))  # Decompress the downloaded .gz file\n",
    "   os.system(\"cp /tmp/{0} {1}\".format(decompressed_file_name, download_path))  # Copy the decompressed file to the final download path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "511d06f0-a864-4adf-adbd-9f3207f9e5ae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Load and Replace Data from TSV Files"
    }
   },
   "outputs": [],
   "source": [
    "for file in dbutils.fs.ls(download_path):\n",
    "    table_name = file.name.replace('.tsv', '').replace('.', '_')  # Create table name from file name\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"  # Construct full table name with catalog and schema\n",
    "    if spark._jsparkSession.catalog().tableExists(full_table_name):  # Check if the table already exists\n",
    "        path = file.path  # Get the file path\n",
    "        df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(path)  # Read the TSV file into a DataFrame\n",
    "        df = df.replace(r\"\\N\", None)  # Replace '\\N' with None in the DataFrame\n",
    "        df.write\\\n",
    "          .mode(\"overwrite\")\\\n",
    "          .option(\"overwriteSchema\", \"true\")\\\n",
    "          .saveAsTable(\"{0}.{1}.{2}\".format(catalog_name, schema_name, table_name))  # Save DataFrame as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7036475794666101,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bf820cf-f78d-456e-a4ab-6d79148c795c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialize and Fetch Box Office Data from 2010 to 2024"
    }
   },
   "outputs": [],
   "source": [
    "json_data = []  # Initialize an empty list to store the JSON data\n",
    "box_office = BoxOffice()  # Create an instance of the BoxOffice class\n",
    "for y in range(2010, 2025):  # Loop through the years from 2010 to 2023\n",
    "    daily_data = box_office.get_yearly(year=y)  # Fetch daily box office data for the year\n",
    "    for d in daily_data:  # Iterate through each day's data\n",
    "        d['Release Date'] = d['Release Date'] + ' ' + str(y)  # Append the year to the release date\n",
    "    json_data.append(daily_data)  # Add the daily data for the year to the json_data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7036475794666101,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464e6a89-6684-4a3f-950a-ffbc1dfaaafd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Define List of Movie Genres"
    }
   },
   "outputs": [],
   "source": [
    "# List of movie genres to be used for filtering and analysis\n",
    "genres = [\n",
    "    'Action',\n",
    "    'Adventure',\n",
    "    'Comedy',\n",
    "    'Drama',\n",
    "    'Fantasy',\n",
    "    'Horror',\n",
    "    'Mystery',\n",
    "    'Romance',\n",
    "    'Science Fiction',\n",
    "    'Thriller',\n",
    "    'Western',\n",
    "    'Animation',\n",
    "    'Crime',\n",
    "    'Documentary',\n",
    "    'Family',\n",
    "    'Musical',\n",
    "    'War',\n",
    "    'Historical',\n",
    "    'Sports',\n",
    "    'Biography'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556682af-8ccd-4224-8f5b-bb7a2317caf6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Transform and Clean OMDB Data for Analysis"
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"omdb_analysis\"  # Define the name of the table to store the analysis results\n",
    "\n",
    "json_p = spark.sparkContext.parallelize(json_data)  # Convert JSON data to an RDD\n",
    "df = spark.read.json(json_p)  # Read the RDD as a DataFrame\n",
    "\n",
    "# Clean 'Gross' and 'Total Gross' columns by removing dollar signs and commas\n",
    "df = df.withColumn(\n",
    "            \"Gross\",\n",
    "            regexp_replace(\"Gross\", \"\\\\$\", \"\"))\\\n",
    "      .withColumn(\n",
    "            \"Gross\",\n",
    "            regexp_replace(\"Gross\", \",\", \"\"))\\\n",
    "      .withColumn(\n",
    "            \"Total Gross\",\n",
    "            regexp_replace(\"Total Gross\", \"\\\\$\", \"\"))\\\n",
    "      .withColumn(\n",
    "            \"Total Gross\",\n",
    "            regexp_replace(\"Total Gross\", \",\", \"\"))\n",
    "\n",
    "# Cast 'Gross' and 'Total Gross' columns to float type\n",
    "df = df.withColumn(\"Gross\", col(\"Gross\").cast(\"float\"))\n",
    "df = df.withColumn(\"Total Gross\", col(\"Total Gross\").cast(\"float\"))\n",
    "\n",
    "# Convert 'Release Date' to date type\n",
    "df = df.withColumn(\"Release Date\", to_date(col(\"Release Date\"), \"MMM d yyyy\"))\n",
    "\n",
    "# SQL query to fetch movie data from IMDb\n",
    "imdb_query = \"\"\"\n",
    "      SELECT\n",
    "            tb.tconst,\n",
    "            tb.primaryTitle,\n",
    "            tb.runtimeMinutes,\n",
    "            tb.genres,\n",
    "            tr.averageRating,\n",
    "            tr.numVotes\n",
    "      FROM data_analysis.imdb_data.title_basics AS tb\n",
    "      INNER JOIN data_analysis.imdb_data.title_ratings AS tr ON tb.tconst = tr.tconst\n",
    "      WHERE \n",
    "            tb.titleType = 'movie'  \n",
    "            AND tb.isAdult = '0'\n",
    "\"\"\"\n",
    "imdb_df = spark.sql(imdb_query)  # Execute the SQL query and store the result in a DataFrame\n",
    "\n",
    "# Split 'genres' column into an array of genres\n",
    "imdb_df = imdb_df.withColumn(\"GenreArray\", split(col(\"genres\"), \",\"))\n",
    "# Find intersection of GenreArray and predefined genres\n",
    "imdb_df = imdb_df.withColumn(\n",
    "    \"Matches\",\n",
    "    array_intersect(\n",
    "        col(\"GenreArray\"),\n",
    "        array(*[lit(g) for g in genres])  \n",
    "    )\n",
    ")\n",
    "# Find intersection of GenreArray and predefined genres\n",
    "imdb_df = imdb_df.withColumn(\"Primary_Genre\", element_at(col(\"Matches\"), 1))  # Get the first matching genre\n",
    "cols_to_drop = [\"Matches\", \"Genres\", \"GenreArray\"]  # Specify columns to be dropped from the DataFrame\n",
    "imdb_df = imdb_df.drop(*cols_to_drop)  # Drop unnecessary columns from the DataFrame\n",
    "\n",
    "df = df.join(imdb_df, df.Release == imdb_df.primaryTitle, \"inner\")  # Join box office data with IMDb data on movie title\n",
    "df = df.withColumnRenamed(\"Release Date\", \"Release_Date\")\\\n",
    "       .withColumnRenamed(\"Gross\", \"Gross_Earnings\")\\\n",
    "       .withColumnRenamed(\"Total Gross\", \"Total_Gross_Earnings\")  # Rename columns for consistency\n",
    "\n",
    "df.write\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .saveAsTable(\"{0}.{1}.{2}\".format(catalog_name, schema_name, table_name))  # Save DataFrame as a table in the specified database"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "imdb_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
