{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6473428c-7d72-47bd-963b-424787d219cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install BeautifulSoup Library"
    }
   },
   "outputs": [],
   "source": [
    "%pip install bs4  # Install BeautifulSoup library for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c9fdc5-137a-40ec-9c9e-af34ca466230",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "# Restart the Python environment to apply changes made by library installations or updates\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e250ca26-99ae-42f7-91fb-72f06f153fb4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Required Libraries for Data Processing"
    }
   },
   "outputs": [],
   "source": [
    "import json  # For handling JSON data\n",
    "import os  # For interacting with the operating system\n",
    "import requests  # For making HTTP requests\n",
    "import urllib.request  # For opening and reading URLs\n",
    "from bs4 import BeautifulSoup  # For parsing HTML and XML documents\n",
    "from delta.tables import DeltaTable  # For working with Delta Lake tables\n",
    "from pyspark.sql import Row  # For creating Spark DataFrame rows\n",
    "from pyspark.sql.functions import col, regexp_replace  # For DataFrame operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13a53b6-d403-4803-be22-d2548a1e28c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Retrieve OMDB API Key from Secrets Manager"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the OMDB API key from Databricks secrets\n",
    "omdbkey = dbutils.secrets.get(scope = \"djsdbsecrets\", key = \"omdbapikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a3dd2d-afc4-4c27-bce1-27c0bda7635a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create IMDB Schema and Set Download Path"
    }
   },
   "outputs": [],
   "source": [
    "download_path = '/Volumes/generaldata/dataanalysis/upload/imdb/'  # Path to download IMDB data\n",
    "url = 'https://datasets.imdbws.com/'  # URL for IMDB datasets\n",
    "catalog_name = \"data_analysis\"  # Catalog name for the database\n",
    "schema_name = \"imdb_data\"  # Schema name for the database\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS {0}.{1};\".format(catalog_name, schema_name))  # Create schema if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3310efb2-ad4e-4b99-9ebb-624712268ca8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download and Decompress Files from HTML List"
    }
   },
   "outputs": [],
   "source": [
    "html_content = requests.get(url).text  # Fetch HTML content from the specified URL\n",
    "soup = BeautifulSoup(html_content, 'html.parser')  # Parse the HTML content using BeautifulSoup\n",
    "items_list = soup.find('ul')  # Locate the unordered list in the parsed HTML\n",
    "for item in items_list.findAll('a'):  # Iterate through all anchor tags within the list\n",
    "   file_name = item.getText()  # Extract the text (file name) from the anchor tag\n",
    "   decompressed_file_name = file_name.replace('.gz', '')  # Remove the .gz extension for the decompressed file name\n",
    "   file_path = item.get('href')  # Get the href attribute (URL) of the anchor tag\n",
    "   dest_download_path = \"/tmp/{}\".format(file_name)  # Define the temporary download path for the file\n",
    "   urllib.request.urlretrieve(file_path, dest_download_path)  # Download the file to the temporary path\n",
    "   os.system('gzip -d {}'.format(dest_download_path))  # Decompress the downloaded .gz file\n",
    "   os.system(\"cp /tmp/{0} {1}\".format(decompressed_file_name, download_path))  # Copy the decompressed file to the final download path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "511d06f0-a864-4adf-adbd-9f3207f9e5ae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Load and Replace Data from TSV Files"
    }
   },
   "outputs": [],
   "source": [
    "for file in dbutils.fs.ls(download_path):\n",
    "    table_name = file.name.replace('.tsv', '').replace('.', '_')  # Create table name from file name\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"  # Construct full table name with catalog and schema\n",
    "    if spark._jsparkSession.catalog().tableExists(full_table_name):  # Check if the table already exists\n",
    "        path = file.path  # Get the file path\n",
    "        df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(path)  # Read the TSV file into a DataFrame\n",
    "        df = df.replace(r\"\\N\", None)  # Replace '\\N' with None in the DataFrame\n",
    "        df.write\\\n",
    "          .mode(\"overwrite\")\\  # Set write mode to overwrite\n",
    "          .option(\"overwriteSchema\", \"true\")\\  # Allow schema to be overwritten\n",
    "          .saveAsTable(\"{0}.{1}.{2}\".format(catalog_name, schema_name, table_name))  # Save DataFrame as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d3acb6-7e31-4f29-a5fb-684c1ae49525",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Query Recent Non-Adult Movies from IMDb Database"
    }
   },
   "outputs": [],
   "source": [
    "imdb_list = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT\n",
    "        tconst\n",
    "        FROM data_analysis.imdb_data.title_basics\n",
    "        WHERE titleType = 'movie'  -- Filter for movie titles\n",
    "        AND isAdult = '0'          -- Exclude adult titles\n",
    "        AND startYear IS NOT NULL  -- Ensure startYear is not null\n",
    "        AND startYear > 2000 AND startYear <= 2024  -- Limit to movies released between 2001 and 2024\n",
    "        AND genres <> 'Documentary'  -- Exclude documentary genres\n",
    "        LIMIT 500  -- Limit the result to 500 entries\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75be36bd-209b-4d4a-8ede-6c0e88e1673f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Fetch Movie Data from OMDB API Using IMDb IDs"
    }
   },
   "outputs": [],
   "source": [
    "json_data = []\n",
    "for imdbid in imdb_list.select('tconst').rdd.flatMap(lambda x: x).collect():  # Iterate over each IMDb ID\n",
    "    imdb_url = f\"http://www.omdbapi.com/?i={imdbid}&apikey={omdbkey}\"  # Construct the API URL for the IMDb ID\n",
    "    response = requests.get(imdb_url)  # Send a GET request to the API\n",
    "    json_data.append(response.json())  # Append the JSON response to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556682af-8ccd-4224-8f5b-bb7a2317caf6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Load JSON Data into Spark DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "rows = [Row(**json.loads(json.dumps(doc))) for doc in json_data]  # Convert JSON data to Spark Row objects\n",
    "df = spark.createDataFrame(rows)  # Create a Spark DataFrame from the Row objects\n",
    "df = df.withColumn(\n",
    "    \"BoxOffice\",\n",
    "    regexp_replace(col(\"BoxOffice\"), \"[$,]\", \"\").cast(\"decimal(10,2)\")\n",
    ")\n",
    "df.write\\\n",
    "   .mode(\"overwrite\")\\\n",
    "   .option(\"overwriteSchema\", \"true\")\\\n",
    "   .saveAsTable(\"data_analysis.imdb_data.omdb_analysis\")  # Save DataFrame as a table in the specified database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adf24e7d-b4db-4364-af75-e4965986a962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7230179188632966,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "imdb_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
