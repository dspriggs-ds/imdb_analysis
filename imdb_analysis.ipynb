{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6473428c-7d72-47bd-963b-424787d219cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install BeautifulSoup Library"
    }
   },
   "outputs": [],
   "source": [
    "%pip install bs4  # Install BeautifulSoup library for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c9fdc5-137a-40ec-9c9e-af34ca466230",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "# Restart the Python environment to apply changes made by library installations or updates\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e250ca26-99ae-42f7-91fb-72f06f153fb4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Required Libraries for Data Processing"
    }
   },
   "outputs": [],
   "source": [
    "import json  # For handling JSON data\n",
    "import os  # For interacting with the operating system\n",
    "import requests  # For making HTTP requests\n",
    "import urllib.request  # For opening and reading URLs\n",
    "from bs4 import BeautifulSoup  # For parsing HTML and XML documents\n",
    "from delta.tables import DeltaTable  # For working with Delta Lake tables\n",
    "from pyspark.sql import Row  # For creating Spark DataFrame rows\n",
    "from pyspark.sql.functions import col, regexp_replace, split,slice, size, when, concat_ws, array_intersect, array, lit, element_at # For DataFrame operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13a53b6-d403-4803-be22-d2548a1e28c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Retrieve OMDB API Key from Secrets Manager"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the OMDB API key from Databricks secrets\n",
    "omdbkey = dbutils.secrets.get(scope = \"djsdbsecrets\", key = \"omdbapikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a34129e-b08a-4551-a19a-09aef92e371d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Database Catalog and Schema"
    }
   },
   "outputs": [],
   "source": [
    "ext_df = spark.sql('DESCRIBE EXTERNAL LOCATION `externaloc`')  # Retrieve metadata for the external location\n",
    "ext_loc = ext_df.select('url').collect()[0][0]  # Extract the URL of the external location from the DataFrame\n",
    "catalog_name = \"data_analysis\"  # Catalog name for the database\n",
    "schema_name = \"imdb_data\"  # Schema name for the database\n",
    "# Create a catalog if it does not exist, specifying a managed location\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS {0} MANAGED LOCATION '{1}'\".format(catalog_name, ext_loc))\n",
    "\n",
    "# Create a schema within the specified catalog if it does not exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS {0}.{1}\".format(catalog_name, schema_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a3dd2d-afc4-4c27-bce1-27c0bda7635a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set IMDB Download Path and URL"
    }
   },
   "outputs": [],
   "source": [
    "download_path = '/Volumes/generaldata/dataanalysis/upload/imdb/'  # Path to download IMDB data\n",
    "url = 'https://datasets.imdbws.com/'  # URL for IMDB datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3310efb2-ad4e-4b99-9ebb-624712268ca8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download and Decompress Files from HTML List"
    }
   },
   "outputs": [],
   "source": [
    "html_content = requests.get(url).text  # Fetch HTML content from the specified URL\n",
    "soup = BeautifulSoup(html_content, 'html.parser')  # Parse the HTML content using BeautifulSoup\n",
    "items_list = soup.find('ul')  # Locate the unordered list in the parsed HTML\n",
    "for item in items_list.findAll('a'):  # Iterate through all anchor tags within the list\n",
    "   file_name = item.getText()  # Extract the text (file name) from the anchor tag\n",
    "   decompressed_file_name = file_name.replace('.gz', '')  # Remove the .gz extension for the decompressed file name\n",
    "   file_path = item.get('href')  # Get the href attribute (URL) of the anchor tag\n",
    "   dest_download_path = \"/tmp/{}\".format(file_name)  # Define the temporary download path for the file\n",
    "   urllib.request.urlretrieve(file_path, dest_download_path)  # Download the file to the temporary path\n",
    "   os.system('gzip -d {}'.format(dest_download_path))  # Decompress the downloaded .gz file\n",
    "   os.system(\"cp /tmp/{0} {1}\".format(decompressed_file_name, download_path))  # Copy the decompressed file to the final download path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "511d06f0-a864-4adf-adbd-9f3207f9e5ae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Load and Replace Data from TSV Files"
    }
   },
   "outputs": [],
   "source": [
    "for file in dbutils.fs.ls(download_path):\n",
    "    table_name = file.name.replace('.tsv', '').replace('.', '_')  # Create table name from file name\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"  # Construct full table name with catalog and schema\n",
    "    if spark._jsparkSession.catalog().tableExists(full_table_name):  # Check if the table already exists\n",
    "        path = file.path  # Get the file path\n",
    "        df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(path)  # Read the TSV file into a DataFrame\n",
    "        df = df.replace(r\"\\N\", None)  # Replace '\\N' with None in the DataFrame\n",
    "        df.write\\\n",
    "          .mode(\"overwrite\")\\\n",
    "          .option(\"overwriteSchema\", \"true\")\\\n",
    "          .saveAsTable(\"{0}.{1}.{2}\".format(catalog_name, schema_name, table_name))  # Save DataFrame as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d3acb6-7e31-4f29-a5fb-684c1ae49525",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Query Recent Non-Adult Movies from IMDb Database"
    }
   },
   "outputs": [],
   "source": [
    "imdb_list = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT\n",
    "        tconst\n",
    "        FROM data_analysis.imdb_data.title_basics\n",
    "        WHERE titleType = 'movie'  -- Filter for movie titles\n",
    "        AND isAdult = '0'          -- Exclude adult titles\n",
    "        AND startYear >= 2020  -- Limit to movies released between 2001 and 2024\n",
    "        LIMIT 2000\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464e6a89-6684-4a3f-950a-ffbc1dfaaafd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Define List of Movie Genres"
    }
   },
   "outputs": [],
   "source": [
    "# List of movie genres to be used for filtering and analysis\n",
    "genres = [\n",
    "    'Action',\n",
    "    'Adventure',\n",
    "    'Comedy',\n",
    "    'Drama',\n",
    "    'Fantasy',\n",
    "    'Horror',\n",
    "    'Mystery',\n",
    "    'Romance',\n",
    "    'Science Fiction',\n",
    "    'Thriller',\n",
    "    'Western',\n",
    "    'Animation',\n",
    "    'Crime',\n",
    "    'Documentary',\n",
    "    'Family',\n",
    "    'Musical',\n",
    "    'War',\n",
    "    'Historical',\n",
    "    'Sports',\n",
    "    'Biography'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75be36bd-209b-4d4a-8ede-6c0e88e1673f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Fetch Movie Data from OMDB API Using IMDb IDs"
    }
   },
   "outputs": [],
   "source": [
    "json_data = []  # Initialize an empty list to store JSON responses from the API\n",
    "for imdbid in imdb_list.select('tconst').rdd.flatMap(lambda x: x).collect():  # Iterate over each IMDb ID\n",
    "    imdb_url = f\"http://www.omdbapi.com/?i={imdbid}&apikey={omdbkey}\"  # Construct the API URL for the IMDb ID\n",
    "    response = requests.get(imdb_url)  # Send a GET request to the API\n",
    "    json_data.append(response.json() if response.status_code == 200 else {})  # Append the JSON response to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556682af-8ccd-4224-8f5b-bb7a2317caf6",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"Poster\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1747708945390}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "- Transform and Clean OMDB Data for Analysis"
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"omdb_analysis\"\n",
    "\n",
    "json_p = spark.sparkContext.parallelize(json_data)\n",
    "df = spark.read.json(json_p)\n",
    "\n",
    "# Clean and convert BoxOffice to decimal\n",
    "df = df.withColumn(\n",
    "    \"BoxOffice\",\n",
    "    regexp_replace(col(\"BoxOffice\"), \"[$,]\", \"\").cast(\"decimal(10,2)\")  \n",
    ")\n",
    "# Clean and convert Runtime to integer\n",
    "df = df.withColumn(\n",
    "    \"Runtime\",\n",
    "    regexp_replace(col(\"Runtime\"), \"[,min]\", \"\").cast(\"integer\")  \n",
    ")\n",
    " # Split Genre string into an array\n",
    "df = df.withColumn(\"GenreArray\", split(col(\"Genre\"), \",\")) \n",
    "# Find intersection of GenreArray and predefined genres\n",
    "df = df.withColumn(\n",
    "    \"Matches\",\n",
    "    array_intersect(\n",
    "        col(\"GenreArray\"),\n",
    "        array(*[lit(g) for g in genres])  \n",
    "    )\n",
    ")\n",
    "# Find intersection of GenreArray and predefined genres\n",
    "df = df.withColumn(\"Primary_Genre\", element_at(col(\"Matches\"), 1))  # Get the first matching genre\n",
    "df = df.withColumn(\"Hybrid_Genre\", concat_ws(\",\", col(\"Matches\")))  # Create a comma-separated string of matched genres\n",
    "\n",
    "cols_to_drop = [\"Matches\", \"Genre\", \"GenreArray\"]  # Specify columns to be dropped from the DataFrame\n",
    "\n",
    "df = df.drop(*cols_to_drop)  # Drop unnecessary columns from the DataFrame\n",
    "df = df.withColumn(\"imdbVotes\", when(col(\"imdbVotes\") == \"N/A\", 0).otherwise(col(\"imdbVotes\")))  # Replace 'N/A' in imdbVotes with 0\n",
    "df = df.withColumn(\"Metascore\", when(col(\"Metascore\") == \"N/A\", 0).otherwise(col(\"Metascore\")))  # Replace 'N/A' in Metascore with 0\n",
    "df = df.withColumn(\"imdbRating\", when(col(\"imdbRating\") == \"N/A\", 0.0).otherwise(col(\"imdbRating\")))  # Replace 'N/A' in imdbRating with 0.0\n",
    "df = df.fillna({'Runtime': 0, 'Primary_Genre': 'unknown', 'Hybrid_Genre': 'unknown'})  # Fill null values in specified columns with default values\n",
    "display(df)\n",
    "\n",
    "\n",
    "# df.write\\\n",
    "#   .mode(\"overwrite\")\\\n",
    "#   .option(\"mergeSchema\", \"true\") \\\n",
    "#   .saveAsTable(\"{0}.{1}.{2}\".format(catalog_name, schema_name, table_name))  # Save DataFrame as a table in the specified database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df02ceaa-fa60-4a67-b384-048d9b424f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShzcGFyay50YWJsZSgiZGF0YV9hbmFseXNpcy5pbWRiX2RhdGEub21kYl9hbmFseXNpcyIpKQ==\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "f2806106-f758-4ac3-aad9-18778e213cfe",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 12.5625,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1747705630070,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.table(\"data_analysis.imdb_data.omdb_analysis\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "imdb_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
